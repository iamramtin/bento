http:
  address: 0.0.0.0:4195
  debug_endpoints: true

input:
  broker:
    inputs:
    - kafka:
        addresses:
        - kafka:9092
        topic: data_source
        target_version: 1.1.0
        max_batch_count: 20

    - type: ditto
      kafka:
        topic: retry_queue
      processors:
      - awk:
          codec: json
          program: |
            {
              delay_for = 30 - (timestamp_unix() - int(metadata_get("last_attempt_at")))
              if ( delay_for < 0 )
                delay_for = 0

              retryAttemptsStr = metadata_get("retry_attempts")
              retryAttempts = 1
              if ( length(retryAttemptsStr) > 0 ) {
                retryAttempts = int(retryAttemptsStr) + 1
              }
              metadata_set("retry_attempts", retryAttempts)
              metadata_set("delay_for_s", delay_for)
            }
      - sleep:
          duration: "${!metadata:delay_for_s}s"

pipeline:
  processors:
  - metadata:
      operator: set
      key: output_topic
      value: output_sink

  - process_dag:
      articles:
        conditions:
        - jmespath:
            query: "type == 'article'"
        processors:
        - cache:
            cache: article_cache
            operator: set
            key: "${!json_field:article.id}"
            value: "${!content}"

      likes:
        conditions:
        - jmespath:
            query: "type == 'like'"
        processors:
        - cache:
            cache: article_cache
            operator: get
            key: "${!json_field:like.article_id}"
        postmap:
          article: article

  # This will catch any of the previous cache operations failing and allows us
  # to redirect these failed documents to a retry queue topic.
  - catch:
    - metadata:
        operator: set
        key: output_topic
        value: retry_queue
    - metadata:
        operator: set
        key: last_attempt_at
        value: "${!timestamp_unix}"
    - conditional:
        condition:
          not:
            metadata:
              operator: greater_than
              key: retry_attempts
              arg: 3
        else_processors:
        - log:
            level: ERROR
            message: "Dropping message '${!content}' due to running out of retry attempts"
        - filter:
            type: static
            static: false

output:
  kafka:
    addresses:
    - kafka:9092
    topic: ${!metadata:output_topic}
    target_version: 1.1.0

resources:
  caches:
    article_cache:
      type: memory

tracer:
  jaeger:
    agent_address: 'jaeger:6831'

logger:
  prefix: service
  level: INFO

metrics:
  prometheus:
    prefix: benthos
